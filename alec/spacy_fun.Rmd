---
title: "named_entity_extraction"
author: "Alec"
date: "10/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup SpaCy


```{r}
library("spacyr")
spacy_install()
```

```{r}
spacy_initialize()
```

## Get Indeed Job Postings

```{r}
library(httr)
library(jsonlite)
library(RCurl)
library(xml2)
library(stringr)
library(rvest)
library(RSelenium)
```

```{r}
ny_jobs <- read_csv("sample_new_york_jobs.csv")
```


```{r}
library(tidyverse)
library(tidytext)
```


```{r}
# get the title of the different speeches
descriptions <- ny_jobs$full_job_text

# create an empty list
list_descriptions <- list()

# loop over the different speeches
for(i in 1:length(descriptions)){

  # use tidytext sentencizer to separate text into sentences.
  sentences <- ny_jobs %>%
    filter(full_job_text == descriptions[i]) %>%
    unnest_tokens(sentence,             # this is the name of the output
                  full_job_text,                 # this is the name of the input
                  token = "sentences",  # this indicates that I want to have sentences as my output
                  drop  = T,            # I want to drop the original text
                  to_lower = F)         # I do not, yet, want to transform to lowercase letters

  # Now save the different sentences in one vector and numerically name it
  all_descriptions <- sentences$sentence
  names(all_descriptions) <- 1:length(all_descriptions)

  # Now I use SpacyR to parse the sentences
  list_descriptions[[i]] <- spacy_parse(all_descriptions,          # input file
                                    lemma = TRUE,      # yes, I want the lemmas
                                    entity = TRUE,    
                                    nounphrase = TRUE, #Yes, I want to separate the text into nounphrases

                                    # some additional attributes that spacyr can return
                                    # include whether a token is punctuation
                                    # or a stop word
                                    additional_attributes = c("is_punct",
                                                              "is_stop")) %>%
    # Now I can just save the output as a tibble
    as_tibble() %>%
    # Delete the sentence ID
    select(-sentence_id) %>%
    # spacyR has named the input as doc_id, but in this case the output ids are sentences
    rename(sentence_nr = doc_id) %>%
    # I want to add the name of the speech as a column
    mutate(speech =  descriptions[i])
}


```

```{r}
parsedtxt <- do.call("rbind", list_descriptions) %>%
  as_tibble() %>%
  # make sure the sentence number is numeric
  mutate(sentence_nr = as.numeric(sentence_nr))
```

```{r}
parsedtxt
```

```{r}
ny_jobs %>%
  # get bi-grams
  unnest_tokens(trigram, full_job_text, token = "ngrams", n = 3, n_min = 2) %>%
  # separate bi0grams into different columns
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  # use tidytext stop word lexicon to filter stop words (the, a, etc.)
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  # count the stop words
  count(word1, word2, word3, sort = TRUE)
```

spacy_finalize()


